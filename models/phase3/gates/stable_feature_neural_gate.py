# models/phase3/gates/stable_feature_neural_gate.py
"""
Stable Feature-Based Neural Gate - PRODUCTION READY

Fixes the high variance issues (94% ‚Üí 92% ‚Üí 54%) in the original neural gates.
Uses ensemble training and conservative techniques for consistent performance.

Key improvements:
- Multiple model ensemble for stability
- Conservative architecture and training
- Feature-based routing (no target needed)
- Robust initialization and regularization
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Tuple, Dict, Any, List
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import r2_score, mean_squared_error
import torch.nn.functional as F


class StableFeatureRoutingGate(nn.Module):
    """
    STABLE neural gate with conservative architecture for consistent performance
    
    Design principles:
    - Small, simple architecture to prevent overfitting
    - Residual connections for gradient flow
    - LayerNorm for training stability
    - Conservative dropout rates
    """
    
    def __init__(self, feature_dim: int, num_bins: int, hidden_dim: int = 64):
        super().__init__()
        
        self.feature_dim = feature_dim
        self.num_bins = num_bins
        
        # CONSERVATIVE architecture - small and stable
        self.encoder = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.LayerNorm(hidden_dim // 4),
            nn.ReLU(),
            nn.Dropout(0.05),
        )
        
        # Simple classifier head
        self.classifier = nn.Linear(hidden_dim // 4, num_bins)
        
        # VERY conservative initialization
        self._init_weights()
    
    def _init_weights(self):
        """Ultra-conservative weight initialization for maximum stability"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # Very small initialization
                nn.init.xavier_uniform_(module.weight, gain=0.1)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, features: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
        """Forward pass with temperature scaling"""
        # Encode features
        encoded = self.encoder(features)
        
        # Get logits
        logits = self.classifier(encoded)
        
        # Apply temperature
        scaled_logits = logits / max(temperature, 0.1)  # Prevent division by zero
        
        # Return probabilities
        return F.softmax(scaled_logits, dim=1)


class EnsembleStableTrainer:
    """
    ENSEMBLE trainer that trains multiple models and averages them for stability
    
    This dramatically reduces variance by combining multiple independent models
    """
    
    def __init__(self, config, logger):
        self.config = config
        self.logger = logger
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Use robust scaling for extra stability
        self.feature_scaler = RobustScaler()
        
        # Training configuration
        self.ensemble_size = 5  # Train 5 models and average
        self.base_lr = 1e-4     # Very conservative learning rate
        self.max_epochs = 100   # Fewer epochs to prevent overfitting
        
    def train_stable_ensemble(self, X_train: np.ndarray, X_val: np.ndarray,
                             y_train: np.ndarray, y_val: np.ndarray,
                             train_bins: np.ndarray, val_bins: np.ndarray,
                             boundaries: np.ndarray, trained_experts: list) -> List[StableFeatureRoutingGate]:
        """
        Train ensemble of stable routing gates
        
        Returns multiple trained models that can be averaged for predictions
        """
        self.logger.info("Training STABLE ENSEMBLE of routing gates...")
        self.logger.info(f"Ensemble size: {self.ensemble_size} models")
        self.logger.info("Goal: Reduce variance through model averaging")
        
        # Store components
        self.boundaries = boundaries
        self.trained_experts = trained_experts
        
        # Scale features once
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        X_val_scaled = self.feature_scaler.transform(X_val)
        
        # Log routing target analysis
        self._log_routing_analysis(train_bins, val_bins, boundaries)
        
        # Train ensemble
        ensemble_models = []
        ensemble_results = []
        
        for model_idx in range(self.ensemble_size):
            self.logger.info(f"\n--- Training Model {model_idx + 1}/{self.ensemble_size} ---")
            
            # Train single model with different random seed
            model, result = self._train_single_stable_model(
                X_train_scaled, X_val_scaled, y_train, y_val,
                train_bins, val_bins, model_idx
            )
            
            if model is not None:
                ensemble_models.append(model)
                ensemble_results.append(result)
                
                self.logger.info(f"Model {model_idx + 1}: "
                               f"Val R¬≤ = {result['val_r2']:.4f}, "
                               f"Routing Acc = {result['routing_accuracy']:.1%}")
        
        if not ensemble_models:
            raise ValueError("Failed to train any stable models!")
        
        # Analyze ensemble performance
        self._analyze_ensemble_performance(ensemble_results)
        
        self.logger.info(f"\nSuccessfully trained {len(ensemble_models)}/{self.ensemble_size} models")
        
        return ensemble_models
    
    def _train_single_stable_model(self, X_train_scaled: np.ndarray, X_val_scaled: np.ndarray,
                                  y_train: np.ndarray, y_val: np.ndarray,
                                  train_bins: np.ndarray, val_bins: np.ndarray,
                                  model_idx: int) -> Tuple[StableFeatureRoutingGate, Dict]:
        """Train a single stable model with specific random seed"""
        
        # Set deterministic seed for this model
        model_seed = 42 + model_idx * 17
        torch.manual_seed(model_seed)
        np.random.seed(model_seed)
        
        # Create model
        n_features = X_train_scaled.shape[1]
        n_bins = len(self.trained_experts)
        
        model = StableFeatureRoutingGate(n_features, n_bins, hidden_dim=64).to(self.device)
        
        # ULTRA-conservative optimizer settings
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=self.base_lr,
            weight_decay=1e-2,  # Strong regularization
            eps=1e-8
        )
        
        # Gentle learning rate schedule
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.max_epochs, eta_min=self.base_lr * 0.1
        )
        
        # Loss with label smoothing for stability
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # Convert to tensors
        X_train_t = torch.from_numpy(X_train_scaled.astype(np.float32)).to(self.device)
        X_val_t = torch.from_numpy(X_val_scaled.astype(np.float32)).to(self.device)
        train_bins_t = torch.from_numpy(train_bins.astype(np.int64)).to(self.device)
        val_bins_t = torch.from_numpy(val_bins.astype(np.int64)).to(self.device)
        
        # Training tracking
        best_val_r2 = -np.inf
        best_state = None
        patience = 0
        max_patience = 20
        
        try:
            for epoch in range(self.max_epochs):
                # Dynamic temperature (slower annealing)
                progress = epoch / self.max_epochs
                temperature = 1.2 * (1 - progress) + 0.8 * progress
                
                # Training phase
                model.train()
                optimizer.zero_grad()
                
                # Forward pass
                bin_probs = model(X_train_t, temperature)
                
                # Loss calculation
                loss = criterion(torch.log(bin_probs + 1e-8), train_bins_t)
                
                # Backward pass with conservative gradient clipping
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
                optimizer.step()
                scheduler.step()
                
                # Validation every 10 epochs to reduce noise
                if epoch % 10 == 0:
                    model.eval()
                    with torch.no_grad():
                        val_bin_probs = model(X_val_t, temperature)
                        
                        # Calculate routing accuracy
                        predicted_bins = torch.argmax(val_bin_probs, dim=1)
                        routing_accuracy = (predicted_bins == val_bins_t).float().mean().item()
                        
                        # Calculate R¬≤ through expert routing
                        val_r2 = self._calculate_routing_r2(
                            val_bin_probs.cpu().numpy(), X_val_scaled, y_val
                        )
                    
                    # Track best model
                    if val_r2 > best_val_r2:
                        best_val_r2 = val_r2
                        patience = 0
                        best_state = model.state_dict().copy()
                        best_routing_accuracy = routing_accuracy
                    else:
                        patience += 1
                        if patience >= max_patience:
                            break
            
            # Load best state
            if best_state is not None:
                model.load_state_dict(best_state)
                
                return model, {
                    'val_r2': best_val_r2,
                    'routing_accuracy': best_routing_accuracy,
                    'model_idx': model_idx,
                    'final_epoch': epoch
                }
            else:
                return None, {'error': 'No improvement found'}
                
        except Exception as e:
            self.logger.warning(f"Model {model_idx + 1} training failed: {e}")
            return None, {'error': str(e)}
    
    def _calculate_routing_r2(self, bin_probs: np.ndarray, X_val: np.ndarray, y_val: np.ndarray) -> float:
        """Calculate R¬≤ by routing through expert predictions"""
        try:
            # Get hard bin assignments
            predicted_bins = np.argmax(bin_probs, axis=1)
            
            # Route predictions through experts
            predictions = np.zeros(len(y_val))
            
            for bin_idx in range(len(self.trained_experts)):
                mask = predicted_bins == bin_idx
                
                if mask.sum() > 0:
                    try:
                        bin_predictions = self.trained_experts[bin_idx].predict(X_val[mask])
                        predictions[mask] = bin_predictions
                    except Exception:
                        # Fallback to mean if expert fails
                        predictions[mask] = np.mean(y_val)
            
            # Calculate R¬≤
            return r2_score(y_val, predictions)
            
        except Exception as e:
            self.logger.warning(f"Failed to calculate routing R¬≤: {e}")
            return -999.0
    
    def _log_routing_analysis(self, train_bins: np.ndarray, val_bins: np.ndarray, boundaries: np.ndarray):
        """Log routing target analysis"""
        n_bins = len(boundaries) + 1
        
        self.logger.info("Routing analysis:")
        self.logger.info(f"  Bin boundaries: {[f'${b:,.0f}' for b in boundaries]}")
        
        # Distribution analysis
        train_counts = [np.sum(train_bins == i) for i in range(n_bins)]
        val_counts = [np.sum(val_bins == i) for i in range(n_bins)]
        
        self.logger.info(f"  Train distribution: {train_counts}")
        self.logger.info(f"  Val distribution: {val_counts}")
        
        # Balance check
        min_count = min(train_counts)
        max_count = max(train_counts)
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
        
        if imbalance_ratio > 3:
            self.logger.warning(f"‚ö†Ô∏è  Imbalanced bins: {imbalance_ratio:.1f}x ratio")
    
    def _analyze_ensemble_performance(self, results: List[Dict]):
        """Analyze ensemble performance statistics"""
        if not results:
            return
        
        r2_scores = [r['val_r2'] for r in results if 'val_r2' in r]
        routing_accs = [r['routing_accuracy'] for r in results if 'routing_accuracy' in r]
        
        if r2_scores:
            mean_r2 = np.mean(r2_scores)
            std_r2 = np.std(r2_scores)
            min_r2 = np.min(r2_scores)
            max_r2 = np.max(r2_scores)
            
            self.logger.info(f"\nEnsemble R¬≤ Analysis:")
            self.logger.info(f"  Mean: {mean_r2:.4f} ¬± {std_r2:.4f}")
            self.logger.info(f"  Range: {min_r2:.4f} to {max_r2:.4f}")
            self.logger.info(f"  Variance: {std_r2:.4f} ({'Low' if std_r2 < 0.02 else 'High'})")
        
        if routing_accs:
            mean_acc = np.mean(routing_accs)
            std_acc = np.std(routing_accs)
            
            self.logger.info(f"\nEnsemble Routing Accuracy:")
            self.logger.info(f"  Mean: {mean_acc:.1%} ¬± {std_acc:.1%}")
    
    def predict_with_ensemble(self, ensemble_models: List[StableFeatureRoutingGate], 
                             X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """Make predictions using ensemble of models"""
        
        # Scale inputs
        X_scaled = self.feature_scaler.transform(X)
        X_t = torch.from_numpy(X_scaled.astype(np.float32)).to(self.device)
        
        # Get predictions from each model
        all_bin_probs = []
        
        for model in ensemble_models:
            model.eval()
            with torch.no_grad():
                # Use low temperature for sharp decisions
                bin_probs = model(X_t, temperature=0.5)
                all_bin_probs.append(bin_probs.cpu().numpy())
        
        # Average ensemble predictions
        ensemble_bin_probs = np.mean(all_bin_probs, axis=0)
        
        # Get hard assignments from averaged probabilities
        predicted_bins = np.argmax(ensemble_bin_probs, axis=1)
        
        # Route through experts
        predictions = np.zeros(len(X))
        
        for bin_idx in range(len(self.trained_experts)):
            mask = predicted_bins == bin_idx
            
            if mask.sum() > 0:
                bin_predictions = self.trained_experts[bin_idx].predict(X[mask])
                predictions[mask] = bin_predictions
        
        return predictions, ensemble_bin_probs


def test_stable_ensemble_gate():
    """Test the stable ensemble neural gate"""
    print("üõ°Ô∏è  TESTING STABLE ENSEMBLE NEURAL GATE")
    print("="*60)
    print("Goal: Fix variance issues (94% ‚Üí 92% ‚Üí 54%) with ensemble training")
    print("="*60)
    
    try:
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        
        from models.phase3.core.config import MoEConfig
        from models.phase3.core.logger import MoELogger
        from models.phase3.data.hybrid_data_manager import HybridDataManager
        from models.phase3.experts.enhanced_expert_factory import EnhancedExpertFactory
        
        # Conservative configuration
        config = MoEConfig(
            min_bin_size=30,
            scaling_method='robust',
            random_state=42,
            gate_epochs=100,
            gate_learning_rate=1e-4
        )
        
        logger = MoELogger(config.log_file)
        
        # Load data
        logger.info("Loading data...")
        data_manager = HybridDataManager(config, logger)
        X, y = data_manager.load_data()
        X, y = data_manager.preprocess_data(X, y)
        X_train, X_val, X_test, y_train, y_val, y_test = data_manager.split_data(X, y)
        
        # Use 3 bins for faster, more stable training
        n_bins = 3
        print(f"\nüîç Testing with {n_bins} bins:")
        
        # Create bins and train experts
        boundaries = np.percentile(y_train, np.linspace(0, 100, n_bins + 1))[1:-1]
        train_bins = np.digitize(y_train, boundaries)
        val_bins = np.digitize(y_val, boundaries)
        test_bins = np.digitize(y_test, boundaries)
        
        # Train specialist experts
        expert_factory = EnhancedExpertFactory(config, logger)
        selected_experts = expert_factory.select_experts_for_bins(
            X_train, y_train, train_bins, n_bins
        )
        trained_experts = expert_factory.train_experts(
            X_train, y_train, train_bins, selected_experts
        )
        
        # Get baseline performance
        baseline_predictions = np.zeros(len(y_test))
        for bin_idx in range(n_bins):
            test_mask = test_bins == bin_idx
            if test_mask.sum() > 0 and bin_idx < len(trained_experts):
                bin_pred = trained_experts[bin_idx].predict(X_test[test_mask])
                baseline_predictions[test_mask] = bin_pred
        
        baseline_r2 = r2_score(y_test, baseline_predictions)
        baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_predictions))
        
        print(f"   Baseline (hard routing): R¬≤ = {baseline_r2:.4f}, RMSE = ${baseline_rmse:,.0f}")
        
        # Train stable ensemble
        print(f"\nüõ°Ô∏è  Training stable ensemble:")
        
        ensemble_trainer = EnsembleStableTrainer(config, logger)
        ensemble_models = ensemble_trainer.train_stable_ensemble(
            X_train, X_val, y_train, y_val,
            train_bins, val_bins, boundaries, trained_experts
        )
        
        # Test ensemble
        ensemble_predictions, ensemble_bin_probs = ensemble_trainer.predict_with_ensemble(
            ensemble_models, X_test
        )
        
        ensemble_r2 = r2_score(y_test, ensemble_predictions)
        ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_predictions))
        
        # Results
        print(f"\nüìä STABILITY COMPARISON:")
        print(f"   Baseline (target-based): R¬≤ = {baseline_r2:.4f}, RMSE = ${baseline_rmse:,.0f}")
        print(f"   Ensemble (feature-based): R¬≤ = {ensemble_r2:.4f}, RMSE = ${ensemble_rmse:,.0f}")
        
        performance_ratio = ensemble_r2 / baseline_r2 if baseline_r2 > 0 else 0
        print(f"   Performance ratio: {performance_ratio:.1%} of baseline")
        
        # Routing analysis
        predicted_test_bins = np.argmax(ensemble_bin_probs, axis=1)
        routing_accuracy = np.mean(predicted_test_bins == test_bins)
        
        print(f"\nüéØ ENSEMBLE ROUTING ANALYSIS:")
        print(f"   Routing accuracy: {routing_accuracy:.1%}")
        print(f"   Ensemble size: {len(ensemble_models)} models")
        
        # Success assessment
        if performance_ratio >= 0.95:
            print(f"\nüéâ EXCELLENT: Stable ensemble matches baseline!")
            print(f"   Ready for production deployment")
        elif performance_ratio >= 0.90:
            print(f"\n‚úÖ GOOD: Stable performance achieved")
            print(f"   Suitable for production with monitoring")
        else:
            print(f"\n‚ö†Ô∏è  Needs improvement")
        
        return {
            'baseline_r2': baseline_r2,
            'ensemble_r2': ensemble_r2,
            'performance_ratio': performance_ratio,
            'routing_accuracy': routing_accuracy,
            'ensemble_size': len(ensemble_models),
            'stability': 'high' if len(ensemble_models) >= 4 else 'medium'
        }
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = test_stable_ensemble_gate()